"""
üìä IRIS ANALYTICS - Monitoring et m√©triques pour Iris API
Collecte des m√©triques business et techniques pour optimisation

Author: Claude Phoenix DevSecOps Guardian
Version: 2.0.0 - Production Analytics
"""

import os
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict
from enum import Enum
import asyncio

from supabase import Client

logger = logging.getLogger(__name__)

class EventType(Enum):
    CHAT_REQUEST = "chat_request"
    CHAT_RESPONSE = "chat_response"
    AUTH_SUCCESS = "auth_success"
    AUTH_FAILURE = "auth_failure"
    RATE_LIMIT_HIT = "rate_limit_hit"
    ERROR = "error"
    USER_UPGRADE = "user_upgrade"

@dataclass
class IrisAnalyticsEvent:
    """√âv√©nement analytics Iris"""
    event_type: EventType
    user_id: Optional[str]
    user_tier: Optional[str]
    timestamp: datetime
    metadata: Dict[str, Any]
    session_id: Optional[str] = None
    ip_hash: Optional[str] = None
    user_agent_hash: Optional[str] = None

class IrisAnalytics:
    """
    Service d'analytics pour Iris API
    Collecte m√©triques business et techniques pour optimisation
    """
    
    def __init__(self, supabase_client: Client):
        self.supabase = supabase_client
        self.batch_events: List[IrisAnalyticsEvent] = []
        self.batch_size = int(os.getenv("ANALYTICS_BATCH_SIZE", "50"))
        self.flush_interval = int(os.getenv("ANALYTICS_FLUSH_INTERVAL", "60"))  # secondes
        self._setup_analytics_tables()
        
        # D√©marrer le flush p√©riodique
        asyncio.create_task(self._periodic_flush())
    
    def _setup_analytics_tables(self):
        """S'assure que les tables analytics existent"""
        try:
            # Table pour les √©v√©nements
            self.supabase.table('iris_events').select('id').limit(1).execute()
        except:
            logger.warning("Table iris_events non trouv√©e, cr√©ation recommend√©e")
            
        try:
            # Table pour les m√©triques agr√©g√©es
            self.supabase.table('iris_metrics_daily').select('id').limit(1).execute() 
        except:
            logger.warning("Table iris_metrics_daily non trouv√©e, cr√©ation recommend√©e")
    
    async def track_event(self, event: IrisAnalyticsEvent):
        """Enregistre un √©v√©nement analytics"""
        try:
            # Anonymisation des donn√©es sensibles
            anonymized_event = self._anonymize_event(event)
            
            # Ajouter au batch
            self.batch_events.append(anonymized_event)
            
            # Flush si batch plein
            if len(self.batch_events) >= self.batch_size:
                await self._flush_events()
                
        except Exception as e:
            logger.error(f"Erreur tracking event: {e}")
    
    def _anonymize_event(self, event: IrisAnalyticsEvent) -> IrisAnalyticsEvent:
        """Anonymise les donn√©es sensibles pour RGPD"""
        import hashlib
        
        # Hash de l'user_id
        anonymized_user_id = None
        if event.user_id:
            anonymized_user_id = hashlib.sha256(f"{event.user_id}_analytics".encode()).hexdigest()[:16]
        
        # Hash de l'IP 
        anonymized_ip = None
        if event.ip_hash:
            anonymized_ip = hashlib.sha256(event.ip_hash.encode()).hexdigest()[:8]
        
        # Nettoyer les m√©tadonn√©es sensibles
        clean_metadata = {}
        for key, value in event.metadata.items():
            if key not in ['email', 'password', 'token', 'personal_info']:
                if isinstance(value, str) and len(value) > 100:
                    clean_metadata[key] = value[:100] + "..."
                else:
                    clean_metadata[key] = value
        
        return IrisAnalyticsEvent(
            event_type=event.event_type,
            user_id=anonymized_user_id,
            user_tier=event.user_tier,
            timestamp=event.timestamp,
            metadata=clean_metadata,
            session_id=event.session_id,
            ip_hash=anonymized_ip,
            user_agent_hash=event.user_agent_hash
        )
    
    async def _flush_events(self):
        """Envoie le batch d'√©v√©nements √† Supabase"""
        if not self.batch_events:
            return
            
        try:
            # Convertir en format Supabase
            events_data = [
                {
                    'event_type': event.event_type.value,
                    'user_hash': event.user_id,
                    'user_tier': event.user_tier,
                    'timestamp': event.timestamp.isoformat(),
                    'metadata': json.dumps(event.metadata),
                    'session_id': event.session_id,
                    'ip_hash': event.ip_hash,
                    'user_agent_hash': event.user_agent_hash
                }
                for event in self.batch_events
            ]
            
            # Ins√©rer en batch
            result = self.supabase.table('iris_events').insert(events_data).execute()
            
            logger.info(f"Analytics: {len(self.batch_events)} √©v√©nements envoy√©s")
            self.batch_events.clear()
            
        except Exception as e:
            logger.error(f"Erreur flush analytics: {e}")
            # Garder les √©v√©nements pour retry
            if len(self.batch_events) > self.batch_size * 2:
                # √âviter l'accumulation excessive
                self.batch_events = self.batch_events[-self.batch_size:]
    
    async def _periodic_flush(self):
        """Flush p√©riodique des √©v√©nements"""
        while True:
            try:
                await asyncio.sleep(self.flush_interval)
                await self._flush_events()
            except Exception as e:
                logger.error(f"Erreur flush p√©riodique: {e}")
    
    # === M√âTHODES DE TRACKING SP√âCIALIS√âES ===
    
    async def track_chat_request(
        self, 
        user_id: str, 
        user_tier: str, 
        message_length: int,
        app_context: Optional[str] = None,
        session_id: Optional[str] = None
    ):
        """Track une requ√™te de chat"""
        await self.track_event(IrisAnalyticsEvent(
            event_type=EventType.CHAT_REQUEST,
            user_id=user_id,
            user_tier=user_tier,
            timestamp=datetime.now(),
            metadata={
                'message_length': message_length,
                'app_context': app_context,
            },
            session_id=session_id
        ))
    
    async def track_chat_response(
        self,
        user_id: str,
        user_tier: str, 
        response_length: int,
        processing_time_ms: int,
        model_used: str,
        confidence: float,
        session_id: Optional[str] = None
    ):
        """Track une r√©ponse de chat"""
        await self.track_event(IrisAnalyticsEvent(
            event_type=EventType.CHAT_RESPONSE,
            user_id=user_id,
            user_tier=user_tier,
            timestamp=datetime.now(),
            metadata={
                'response_length': response_length,
                'processing_time_ms': processing_time_ms,
                'model_used': model_used,
                'confidence': confidence,
            },
            session_id=session_id
        ))
    
    async def track_auth_failure(self, reason: str, ip_hash: Optional[str] = None):
        """Track un √©chec d'authentification"""
        await self.track_event(IrisAnalyticsEvent(
            event_type=EventType.AUTH_FAILURE,
            user_id=None,
            user_tier=None,
            timestamp=datetime.now(),
            metadata={'reason': reason},
            ip_hash=ip_hash
        ))
    
    async def track_rate_limit_hit(self, user_id: str, user_tier: str):
        """Track un d√©passement de limite"""
        await self.track_event(IrisAnalyticsEvent(
            event_type=EventType.RATE_LIMIT_HIT,
            user_id=user_id,
            user_tier=user_tier,
            timestamp=datetime.now(),
            metadata={'tier': user_tier}
        ))
    
    async def track_error(
        self, 
        error_type: str, 
        error_message: str,
        user_id: Optional[str] = None,
        user_tier: Optional[str] = None
    ):
        """Track une erreur syst√®me"""
        await self.track_event(IrisAnalyticsEvent(
            event_type=EventType.ERROR,
            user_id=user_id,
            user_tier=user_tier,
            timestamp=datetime.now(),
            metadata={
                'error_type': error_type,
                'error_message': error_message[:200]  # Limit√© pour la DB
            }
        ))
    
    # === M√âTRIQUES BUSINESS ===
    
    async def get_daily_metrics(self, date: datetime) -> Dict[str, Any]:
        """R√©cup√®re les m√©triques d'une journ√©e"""
        try:
            date_str = date.strftime('%Y-%m-%d')
            
            result = self.supabase.table('iris_metrics_daily').select('*').eq('date', date_str).single().execute()
            
            if result.data:
                return result.data
            else:
                # Calculer √† la vol√©e si pas en cache
                return await self._calculate_daily_metrics(date)
                
        except Exception as e:
            logger.error(f"Erreur r√©cup√©ration m√©triques: {e}")
            return {}
    
    async def _calculate_daily_metrics(self, date: datetime) -> Dict[str, Any]:
        """Calcule les m√©triques d'une journ√©e √† partir des √©v√©nements"""
        try:
            date_start = date.strftime('%Y-%m-%d 00:00:00')
            date_end = date.strftime('%Y-%m-%d 23:59:59')
            
            # Requ√™te des √©v√©nements de la journ√©e
            events = self.supabase.table('iris_events').select('*').gte('timestamp', date_start).lte('timestamp', date_end).execute()
            
            if not events.data:
                return {}
            
            # Calculs
            total_requests = len([e for e in events.data if e['event_type'] == 'chat_request'])
            total_responses = len([e for e in events.data if e['event_type'] == 'chat_response'])
            unique_users = len(set([e['user_hash'] for e in events.data if e['user_hash']]))
            
            # R√©partition par tier
            tier_breakdown = {}
            for event in events.data:
                if event['user_tier']:
                    tier = event['user_tier']
                    tier_breakdown[tier] = tier_breakdown.get(tier, 0) + 1
            
            # Temps de r√©ponse moyen
            response_times = [
                json.loads(e['metadata']).get('processing_time_ms', 0)
                for e in events.data
                if e['event_type'] == 'chat_response' and e['metadata']
            ]
            avg_response_time = sum(response_times) / len(response_times) if response_times else 0
            
            metrics = {
                'date': date.strftime('%Y-%m-%d'),
                'total_requests': total_requests,
                'total_responses': total_responses,
                'unique_users': unique_users,
                'avg_response_time_ms': round(avg_response_time, 2),
                'tier_breakdown': tier_breakdown,
                'calculated_at': datetime.now().isoformat()
            }
            
            # Sauvegarder en cache
            self.supabase.table('iris_metrics_daily').upsert(metrics, on_conflict='date').execute()
            
            return metrics
            
        except Exception as e:
            logger.error(f"Erreur calcul m√©triques: {e}")
            return {}
    
    async def get_user_analytics(self, user_id: str, days: int = 7) -> Dict[str, Any]:
        """R√©cup√®re les analytics d'un utilisateur"""
        try:
            # Hash de l'user_id comme dans les √©v√©nements
            import hashlib
            user_hash = hashlib.sha256(f"{user_id}_analytics".encode()).hexdigest()[:16]
            
            # R√©cup√©rer les √©v√©nements des derniers jours
            since_date = (datetime.now() - timedelta(days=days)).isoformat()
            
            events = self.supabase.table('iris_events').select('*').eq('user_hash', user_hash).gte('timestamp', since_date).execute()
            
            if not events.data:
                return {'message_count': 0, 'days_active': 0}
            
            # Calculs
            message_count = len([e for e in events.data if e['event_type'] == 'chat_request'])
            
            # Jours actifs
            active_dates = set([
                datetime.fromisoformat(e['timestamp']).strftime('%Y-%m-%d')
                for e in events.data
            ])
            
            return {
                'message_count': message_count,
                'days_active': len(active_dates),
                'last_activity': max([e['timestamp'] for e in events.data]) if events.data else None,
                'total_events': len(events.data)
            }
            
        except Exception as e:
            logger.error(f"Erreur analytics utilisateur: {e}")
            return {'error': str(e)}

# Fonction pour cr√©er l'instance analytics
def create_analytics(supabase_client: Client) -> IrisAnalytics:
    """Cr√©e une instance d'analytics"""
    return IrisAnalytics(supabase_client)