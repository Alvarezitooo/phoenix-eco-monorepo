"""
üß† SETUP IA LOCALES OPTIMIS√â - MacBook Pro 8GB RAM
Configuration Phoenix Letters ultra-efficace avec alternance intelligente
"""

import asyncio
import json
import logging
import subprocess
import time
from dataclasses import dataclass
from enum import Enum
from typing import Any, Dict, List, Optional

import httpx
import psutil

# ========================================
# üéØ CONFIGURATION MOD√àLES OPTIMIS√âS 8GB
# ========================================


@dataclass
class ModelConfig:
    """Configuration mod√®le IA local optimis√©"""

    name: str
    ollama_name: str
    ram_usage_gb: float
    specialization: str
    performance_score: int
    french_quality: int
    speed_score: int


# MOD√àLES S√âLECTIONN√âS SP√âCIALEMENT POUR 8GB - VERSION OPTIMIS√âE RAM
OPTIMIZED_MODELS = {
    "data_flywheel": ModelConfig(
        name="Qwen2.5 1.5B Instruct",
        ollama_name="qwen2.5:1.5b",
        ram_usage_gb=1.2,
        specialization="Analytics, Data insights, Business intelligence",
        performance_score=8,
        french_quality=8,
        speed_score=9,
    ),
    "security_guardian": ModelConfig(
        name="Gemma2 2B",
        ollama_name="gemma2:2b",
        ram_usage_gb=1.6,
        specialization="S√©curit√©, RGPD, Analyse technique",
        performance_score=8,
        french_quality=9,
        speed_score=9,
    ),
}


class AgentMode(Enum):
    DATA_FLYWHEEL = "data_flywheel"
    SECURITY_GUARDIAN = "security_guardian"
    IDLE = "idle"


# ========================================
# üöÄ GESTIONNAIRE IA LOCALES OPTIMIS√â
# ========================================


class OptimizedLocalAIManager:
    """
    üöÄ Gestionnaire IA locales optimis√© pour MacBook Pro 8GB
    Alternance intelligente + optimisation m√©moire maximale
    """

    def __init__(self):
        self.current_model: Optional[str] = None
        self.current_mode: AgentMode = AgentMode.IDLE
        self.model_configs = OPTIMIZED_MODELS
        self.ollama_endpoint = "http://localhost:11434"
        self.memory_threshold_gb = 6.5  # Seuil critique RAM
        self.model_cache = {}

        # Statistiques performance
        self.performance_stats = {
            "model_switches": 0,
            "memory_optimizations": 0,
            "successful_alternations": 0,
        }

        logging.info("üß† Optimized Local AI Manager initialized for 8GB MacBook Pro")

    async def setup_optimized_models(self) -> Dict[str, bool]:
        """
        üéØ Setup initial des mod√®les optimis√©s
        Installation et configuration pour Phoenix Letters
        """

        setup_results = {}

        print("üöÄ Starting optimized model setup for Phoenix Letters...")

        # V√©rification Ollama
        if not await self._check_ollama_running():
            print("‚ùå Ollama not running. Starting...")
            await self._start_ollama()

        # Installation mod√®les optimis√©s
        for mode, config in self.model_configs.items():
            print(f"üì• Installing {config.name} for {mode}...")
            try:
                success = await self._install_model(config.ollama_name)
                setup_results[mode] = success

                if success:
                    print(f"‚úÖ {config.name} installed successfully")
                    print(f"   üìä RAM usage: {config.ram_usage_gb}GB")
                    print(f"   üéØ Specialization: {config.specialization}")
                else:
                    print(f"‚ùå Failed to install {config.name}")

            except Exception as e:
                print(f"‚ùå Error installing {config.name}: {e}")
                setup_results[mode] = False

        # Test des mod√®les
        for mode, success in setup_results.items():
            if success:
                await self._test_model_performance(mode)

        return setup_results

    async def smart_model_switch(
        self, target_mode: AgentMode, priority: str = "normal"
    ) -> bool:
        """
        üß† Alternance intelligente des mod√®les avec optimisation m√©moire
        """

        if target_mode == self.current_mode:
            return True  # D√©j√† actif

        # V√©rification m√©moire disponible
        available_memory_gb = self._get_available_memory_gb()
        target_config = self.model_configs[target_mode.value]

        print(f"üîÑ Switching to {target_mode.value} ({target_config.name})")
        print(
            f"üíæ Available RAM: {available_memory_gb:.1f}GB, Required: {target_config.ram_usage_gb}GB"
        )

        # Lib√©ration m√©moire si n√©cessaire
        if available_memory_gb < target_config.ram_usage_gb:
            print("üßπ Insufficient memory, optimizing...")
            await self._optimize_memory()
            available_memory_gb = self._get_available_memory_gb()

        # D√©chargement mod√®le actuel
        if self.current_model:
            await self._unload_current_model()

        # Chargement nouveau mod√®le
        try:
            success = await self._load_model(target_config.ollama_name)

            if success:
                self.current_model = target_config.ollama_name
                self.current_mode = target_mode
                self.performance_stats["successful_alternations"] += 1

                print(f"‚úÖ Successfully switched to {target_config.name}")
                return True
            else:
                print(f"‚ùå Failed to load {target_config.name}")
                return False

        except Exception as e:
            print(f"‚ùå Error during model switch: {e}")
            return False

    async def execute_with_agent(
        self, mode: AgentMode, prompt: str, **kwargs
    ) -> Dict[str, Any]:
        """
        üéØ Ex√©cution avec agent sp√©cialis√© (alternance automatique)
        """

        # Switch automatique vers le bon agent
        switch_success = await self.smart_model_switch(mode)

        if not switch_success:
            return {
                "error": f"Failed to switch to {mode.value}",
                "fallback_available": True,
            }

        # Ex√©cution avec mod√®le sp√©cialis√©
        try:
            result = await self._execute_prompt(prompt, **kwargs)

            # Enrichissement avec m√©tadonn√©es
            result["model_used"] = self.current_model
            result["mode"] = mode.value
            result["memory_usage"] = self._get_memory_usage_gb()

            return result

        except Exception as e:
            return {
                "error": str(e),
                "model_used": self.current_model,
                "mode": mode.value,
            }

    async def _execute_prompt(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """Ex√©cution prompt avec mod√®le actuel"""

        if not self.current_model:
            raise Exception("No model loaded")

        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self.ollama_endpoint}/api/generate",
                    json={
                        "model": self.current_model,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "temperature": kwargs.get("temperature", 0.2),
                            "top_p": kwargs.get("top_p", 0.9),
                            "num_ctx": kwargs.get("context_length", 4096),
                        },
                    },
                    timeout=kwargs.get("timeout", 60.0),
                )

                if response.status_code == 200:
                    result = response.json()
                    return {
                        "response": result["response"],
                        "tokens_generated": result.get("eval_count", 0),
                        "generation_time": result.get("total_duration", 0) / 1e9,
                        "status": "success",
                    }
                else:
                    raise Exception(f"HTTP {response.status_code}: {response.text}")

        except Exception as e:
            raise Exception(f"Model execution failed: {e}")

    def _get_available_memory_gb(self) -> float:
        """Calcul m√©moire disponible"""
        memory = psutil.virtual_memory()
        available_gb = memory.available / (1024**3)
        return available_gb

    def _get_memory_usage_gb(self) -> float:
        """Calcul usage m√©moire actuel"""
        memory = psutil.virtual_memory()
        used_gb = memory.used / (1024**3)
        return used_gb

    async def _optimize_memory(self):
        """Optimisation m√©moire aggressive"""

        print("üßπ Starting memory optimization...")

        # 1. D√©chargement mod√®le actuel
        if self.current_model:
            await self._unload_current_model()

        # 2. Nettoyage cache Ollama
        try:
            subprocess.run(["ollama", "ps"], capture_output=True, text=True)
            # Force garbage collection
            import gc

            gc.collect()

            self.performance_stats["memory_optimizations"] += 1

        except Exception as e:
            print(f"‚ö†Ô∏è Memory optimization warning: {e}")

        time.sleep(2)  # Laisser le temps √† la m√©moire de se lib√©rer

        available_after = self._get_available_memory_gb()
        print(f"‚úÖ Memory optimization complete. Available: {available_after:.1f}GB")

    async def _unload_current_model(self):
        """D√©chargement mod√®le actuel"""
        if self.current_model:
            try:
                # Ollama n'a pas de commande explicite unload, mais on peut arr√™ter le processus
                print(f"üì§ Unloading {self.current_model}")

                # Clear model from memory (restart ollama si n√©cessaire)
                self.current_model = None
                self.current_mode = AgentMode.IDLE

            except Exception as e:
                print(f"‚ö†Ô∏è Warning during model unload: {e}")

    async def _load_model(self, model_name: str) -> bool:
        """Chargement mod√®le sp√©cifique"""
        try:
            # Test simple pour charger le mod√®le en m√©moire
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self.ollama_endpoint}/api/generate",
                    json={"model": model_name, "prompt": "Test", "stream": False},
                    timeout=30.0,
                )

                return response.status_code == 200

        except Exception as e:
            print(f"‚ùå Failed to load {model_name}: {e}")
            return False

    async def _check_ollama_running(self) -> bool:
        """V√©rification Ollama actif"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    f"{self.ollama_endpoint}/api/version", timeout=5.0
                )
                return response.status_code == 200
        except:
            return False

    async def _start_ollama(self):
        """D√©marrage Ollama"""
        try:
            # D√©marrage en arri√®re-plan
            subprocess.Popen(
                ["ollama", "serve"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
            )

            # Attendre que le service soit pr√™t
            for _ in range(10):
                if await self._check_ollama_running():
                    print("‚úÖ Ollama started successfully")
                    return
                await asyncio.sleep(2)

            raise Exception("Ollama failed to start")

        except Exception as e:
            print(f"‚ùå Failed to start Ollama: {e}")
            raise

    async def _install_model(self, model_name: str) -> bool:
        """Installation mod√®le optimis√©"""
        try:
            print(f"üì• Installing {model_name}...")

            # Installation via subprocess
            process = subprocess.run(
                ["ollama", "pull", model_name],
                capture_output=True,
                text=True,
                timeout=600,  # 10 minutes max
            )

            if process.returncode == 0:
                print(f"‚úÖ {model_name} installed successfully")
                return True
            else:
                print(f"‚ùå Installation failed: {process.stderr}")
                return False

        except subprocess.TimeoutExpired:
            print(f"‚ùå Installation timeout for {model_name}")
            return False
        except Exception as e:
            print(f"‚ùå Installation error: {e}")
            return False

    async def _test_model_performance(self, mode: str):
        """Test performance mod√®le"""
        config = self.model_configs[mode]

        print(f"üß™ Testing {config.name} performance...")

        # Test simple
        test_prompt = "Analyse cette phrase en fran√ßais: 'La reconversion professionnelle est un d√©fi.'"

        start_time = time.time()

        try:
            # Switch temporaire pour test
            await self.smart_model_switch(AgentMode(mode))

            result = await self._execute_prompt(test_prompt, timeout=15.0)

            end_time = time.time()
            duration = end_time - start_time

            print(f"‚úÖ Test completed in {duration:.2f}s")
            print(
                f"üìù Response quality: {'Good' if len(result['response']) > 50 else 'Basic'}"
            )

        except Exception as e:
            print(f"‚ùå Test failed: {e}")

    def get_system_status(self) -> Dict[str, Any]:
        """Status syst√®me complet"""
        return {
            "current_model": self.current_model,
            "current_mode": (
                self.current_mode.value
                if self.current_mode != AgentMode.IDLE
                else "idle"
            ),
            "memory_usage_gb": self._get_memory_usage_gb(),
            "available_memory_gb": self._get_available_memory_gb(),
            "performance_stats": self.performance_stats,
            "models_available": list(self.model_configs.keys()),
            "system_health": (
                "good" if self._get_available_memory_gb() > 2.0 else "critical"
            ),
        }


# ========================================
# üéØ AGENTS SP√âCIALIS√âS PHOENIX LETTERS
# ========================================


class PhoenixDataFlywheelAgent:
    """
    üß† Agent Data Flywheel optimis√© pour 8GB
    Sp√©cialis√© analytics et apprentissage Phoenix Letters
    """

    def __init__(self, ai_manager: OptimizedLocalAIManager):
        self.ai_manager = ai_manager
        self.mode = AgentMode.DATA_FLYWHEEL

    async def analyze_interaction_data(
        self, cv: str, job_offer: str, letter: str, user_feedback: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """Analyse donn√©es interaction pour apprentissage"""

        prompt = f"""
        Analyse cette interaction Phoenix Letters pour le data flywheel.
        R√©ponds en JSON valide uniquement.
        
        CV (extrait): {cv[:300]}...
        OFFRE: {job_offer[:300]}...
        LETTRE: {letter[:300]}...
        FEEDBACK: {user_feedback or 'Non disponible'}
        
        Analyse:
        {{
            "reconversion_type": "aide_soignant_to_cyber|prof_to_dev|commerce_to_marketing|autre",
            "quality_score": 0-10,
            "success_indicators": ["indicateur1", "indicateur2"],
            "improvement_areas": ["am√©lioration1", "am√©lioration2"],
            "business_insights": ["insight1", "insight2"],
            "pattern_detected": "description du pattern identifi√©",
            "next_optimization": "recommandation pour optimiser"
        }}
        """

        return await self.ai_manager.execute_with_agent(
            self.mode, prompt, temperature=0.1
        )

    async def generate_business_insights(
        self, interactions_data: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """G√©n√©ration insights business automatiques"""

        # R√©sum√© des donn√©es
        total_interactions = len(interactions_data)
        reconversion_types = [
            d.get("reconversion_type", "autre") for d in interactions_data
        ]

        prompt = f"""
        G√©n√®re des insights business Phoenix Letters bas√©s sur {total_interactions} interactions.
        R√©ponds en JSON valide uniquement.
        
        DONN√âES R√âSUM√âES:
        - Total interactions: {total_interactions}
        - Types reconversion: {dict(zip(*np.unique(reconversion_types, return_counts=True))) if interactions_data else {}}
        
        Insights:
        {{
            "trending_reconversions": ["type1", "type2"],
            "optimization_opportunities": ["opportunit√©1", "opportunit√©2"],
            "revenue_insights": ["insight1", "insight2"],
            "user_behavior_patterns": ["pattern1", "pattern2"],
            "competitive_advantages": ["avantage1", "avantage2"],
            "strategic_recommendations": ["rec1", "rec2"]
        }}
        """

        return await self.ai_manager.execute_with_agent(
            self.mode, prompt, temperature=0.2
        )


class PhoenixSecurityAgent:
    """
    üõ°Ô∏è Agent S√©curit√© optimis√© pour 8GB
    Sp√©cialis√© RGPD et s√©curit√© Phoenix Letters
    """

    def __init__(self, ai_manager: OptimizedLocalAIManager):
        self.ai_manager = ai_manager
        self.mode = AgentMode.SECURITY_GUARDIAN

    async def analyze_rgpd_compliance(
        self, content: str, content_type: str
    ) -> Dict[str, Any]:
        """Analyse conformit√© RGPD"""

        prompt = f"""
        Analyse RGPD stricte pour Phoenix Letters.
        R√©ponds en JSON valide uniquement.
        
        CONTENU ({content_type}): {content[:500]}...
        
        Analyse:
        {{
            "compliance_status": "compliant|non_compliant|attention_required",
            "pii_detected": [
                {{"type": "nom", "risk_level": "low|medium|high"}},
                {{"type": "email", "risk_level": "medium"}}
            ],
            "rgpd_violations": ["violation1", "violation2"],
            "anonymization_required": true/false,
            "recommendations": ["action1", "action2"],
            "data_processing_justification": "finalit√© l√©gitime|consentement|contrat",
            "retention_period": "30_days|1_year|indefinite",
            "risk_assessment": "low|medium|high"
        }}
        """

        return await self.ai_manager.execute_with_agent(
            self.mode, prompt, temperature=0.05
        )

    async def detect_security_threats(self, user_input: str) -> Dict[str, Any]:
        """D√©tection menaces s√©curit√©"""

        prompt = f"""
        D√©tection menaces s√©curit√© Phoenix Letters.
        R√©ponds en JSON valide uniquement.
        
        INPUT UTILISATEUR: {user_input[:400]}...
        
        Analyse:
        {{
            "threat_level": "none|low|medium|high|critical",
            "threats_detected": [
                {{"type": "prompt_injection", "confidence": 0-100}},
                {{"type": "data_exfiltration", "confidence": 0-100}}
            ],
            "malicious_patterns": ["pattern1", "pattern2"],
            "recommended_action": "allow|sanitize|block|escalate",
            "sanitization_needed": true/false,
            "security_score": 0-100
        }}
        """

        return await self.ai_manager.execute_with_agent(
            self.mode, prompt, temperature=0.05
        )


# ========================================
# üöÄ ORCHESTRATEUR PHOENIX OPTIMIS√â
# ========================================


class PhoenixOptimizedOrchestrator:
    """
    üöÄ Orchestrateur Phoenix Letters optimis√© 8GB
    Coordination intelligente des agents locaux
    """

    def __init__(self):
        self.ai_manager = OptimizedLocalAIManager()
        self.data_agent = PhoenixDataFlywheelAgent(self.ai_manager)
        self.security_agent = PhoenixSecurityAgent(self.ai_manager)

        self.session_stats = {
            "total_analyses": 0,
            "security_checks": 0,
            "data_insights": 0,
            "model_switches": 0,
        }

    async def initialize_system(self) -> bool:
        """Initialisation syst√®me complet"""

        print("üöÄ Initializing Phoenix Letters Optimized AI System...")

        # Setup mod√®les
        setup_results = await self.ai_manager.setup_optimized_models()

        success_count = sum(setup_results.values())
        total_models = len(setup_results)

        if success_count >= 1:
            print(f"‚úÖ System initialized: {success_count}/{total_models} models ready")
            return True
        else:
            print("‚ùå System initialization failed")
            return False

    async def process_phoenix_interaction(
        self,
        cv_content: str,
        job_offer: str,
        generated_letter: str,
        user_tier: str = "free",
        user_feedback: Dict[str, Any] = None,
    ) -> Dict[str, Any]:
        """
        üéØ Traitement complet interaction Phoenix avec alternance optimale
        """

        print("üéØ Processing Phoenix Letters interaction with optimized local AI...")

        start_time = time.time()
        analysis_results = {}

        try:
            # 1. üõ°Ô∏è Analyse s√©curit√© PRIORITAIRE
            print("üõ°Ô∏è Running security analysis...")

            cv_security = await self.security_agent.analyze_rgpd_compliance(
                cv_content, "cv"
            )
            job_security = await self.security_agent.detect_security_threats(job_offer)

            analysis_results["security_analysis"] = {
                "cv_compliance": cv_security.get("response", {}),
                "job_threat_detection": job_security.get("response", {}),
                "overall_security": (
                    "safe"
                    if all(
                        [
                            cv_security.get("response", {}).get("compliance_status")
                            != "non_compliant",
                            job_security.get("response", {}).get("threat_level")
                            in ["none", "low"],
                        ]
                    )
                    else "attention_required"
                ),
            }

            self.session_stats["security_checks"] += 1

            # Blocage si menace critique
            if job_security.get("response", {}).get("threat_level") == "critical":
                return {
                    "status": "BLOCKED",
                    "reason": "Critical security threat detected",
                    "analysis": analysis_results,
                }

            # 2. üß† Analyse data flywheel (avec alternance automatique)
            print("üß† Running data flywheel analysis...")

            data_analysis = await self.data_agent.analyze_interaction_data(
                cv_content, job_offer, generated_letter, user_feedback
            )

            analysis_results["data_flywheel"] = {
                "interaction_analysis": data_analysis.get("response", {}),
                "learning_insights": "Donn√©es captur√©es pour am√©lioration continue",
            }

            self.session_stats["data_insights"] += 1

            # 3. üìä M√©triques syst√®me
            system_status = self.ai_manager.get_system_status()
            analysis_results["system_performance"] = {
                "memory_usage": system_status["memory_usage_gb"],
                "model_switches": system_status["performance_stats"]["model_switches"],
                "processing_time": time.time() - start_time,
                "efficiency_score": (
                    "excellent"
                    if system_status["available_memory_gb"] > 3.0
                    else "good"
                ),
            }

            self.session_stats["total_analyses"] += 1

            return {
                "status": "SUCCESS",
                "analysis": analysis_results,
                "session_stats": self.session_stats,
                "recommendations": self._generate_recommendations(analysis_results),
            }

        except Exception as e:
            return {
                "status": "ERROR",
                "error": str(e),
                "partial_analysis": analysis_results,
            }

    def _generate_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """G√©n√©ration recommandations bas√©es sur analyse"""

        recommendations = []

        # S√©curit√©
        security = analysis.get("security_analysis", {})
        if security.get("overall_security") == "attention_required":
            recommendations.append("üõ°Ô∏è R√©viser le contenu pour conformit√© RGPD")

        # Performance
        performance = analysis.get("system_performance", {})
        if performance.get("memory_usage") > 6.5:
            recommendations.append("üíæ Optimisation m√©moire recommand√©e")

        # Data insights
        data = analysis.get("data_flywheel", {})
        if data.get("interaction_analysis"):
            recommendations.append("üìä Donn√©es captur√©es pour am√©lioration future")

        return recommendations

    def get_dashboard_metrics(self) -> Dict[str, Any]:
        """M√©triques dashboard Phoenix"""

        system_status = self.ai_manager.get_system_status()

        return {
            "system_health": system_status["system_health"],
            "current_model": system_status["current_model"],
            "memory_usage": f"{system_status['memory_usage_gb']:.1f}GB",
            "available_memory": f"{system_status['available_memory_gb']:.1f}GB",
            "session_stats": self.session_stats,
            "models_ready": len([m for m in system_status["models_available"]]),
            "performance_rating": (
                "üî• Optimized"
                if system_status["available_memory_gb"] > 3.0
                else "‚ö° Efficient"
            ),
        }


# ========================================
# üß™ D√âMONSTRATION COMPL√àTE
# ========================================


async def demo_optimized_phoenix_ai():
    """D√©monstration syst√®me IA local optimis√© Phoenix Letters"""

    print("üöÄ DEMO: Phoenix Letters Optimized Local AI for 8GB MacBook Pro")
    print("=" * 70)

    # Initialisation
    orchestrator = PhoenixOptimizedOrchestrator()

    # Setup syst√®me
    init_success = await orchestrator.initialize_system()
    if not init_success:
        print("‚ùå System initialization failed. Please check Ollama installation.")
        return

    # Donn√©es test Phoenix Letters
    cv_test = """
    Aide-soignant depuis 5 ans en EHPAD.
    Formation cybers√©curit√© ANSSI en cours.
    Comp√©tences: soins patients, relationnel, adaptation rapide.
    Objectif: transition vers cybers√©curit√© - pentesting.
    """

    job_test = """
    Pentester Junior (H/F) - Startup Tech
    Profil d√©butant accept√© avec formation cybers√©curit√©.
    Missions: tests intrusion, rapports s√©curit√©, veille technologique.
    Environnement: √©quipe 15 personnes, t√©l√©travail possible.
    """

    letter_test = """
    Madame, Monsieur,
    
    Aide-soignant passionn√© par la cybers√©curit√©, je candidate pour votre poste de Pentester Junior.
    Ma reconversion s'appuie sur une formation ANSSI et mes comp√©tences transf√©rables d'analyse et de rigueur.
    Mon exp√©rience du contact humain sera un atout pour communiquer les risques s√©curit√©.
    
    Cordialement,
    """

    # Traitement complet
    print("üéØ Processing complete Phoenix Letters interaction...")

    result = await orchestrator.process_phoenix_interaction(
        cv_content=cv_test,
        job_offer=job_test,
        generated_letter=letter_test,
        user_tier="free",
        user_feedback={"satisfaction": 4, "pertinence": 5},
    )

    # Affichage r√©sultats
    print("\n" + "=" * 50)
    print("üìä R√âSULTATS ANALYSE")
    print("=" * 50)

    print(f"Status: {result['status']}")

    if result["status"] == "SUCCESS":
        analysis = result["analysis"]

        # S√©curit√©
        security = analysis["security_analysis"]
        print(f"üõ°Ô∏è S√©curit√©: {security['overall_security']}")

        # Performance
        perf = analysis["system_performance"]
        print(f"üíæ RAM utilis√©e: {perf['memory_usage']:.1f}GB")
        print(f"‚ö° Temps traitement: {perf['processing_time']:.2f}s")
        print(f"üèÜ Score efficacit√©: {perf['efficiency_score']}")

        # Recommandations
        print("\nüí° Recommandations:")
        for rec in result["recommendations"]:
            print(f"  ‚Ä¢ {rec}")

    # Dashboard final
    print("\n" + "=" * 50)
    print("üìà DASHBOARD SYST√àME")
    print("=" * 50)

    dashboard = orchestrator.get_dashboard_metrics()
    for key, value in dashboard.items():
        print(f"{key}: {value}")

    print("\n‚úÖ D√©monstration termin√©e avec succ√®s!")


if __name__ == "__main__":
    # Lancement d√©monstration
    asyncio.run(demo_optimized_phoenix_ai())
