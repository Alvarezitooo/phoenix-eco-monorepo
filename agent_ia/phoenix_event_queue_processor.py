"""
üéØ PHOENIX EVENT QUEUE PROCESSOR - Architecture Hybride Parfaite
Traitement intelligent des √©v√©nements en batch avec d√©duplication garantie
Z√©ro doublon, z√©ro perte, performance optimale
"""

import asyncio
import hashlib
import logging
import os
import time
import uuid
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Set, Tuple
from dataclasses import dataclass
from enum import Enum

from supabase import Client, create_client
import structlog

# Configuration logging structur√©
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="ISO"),
        structlog.processors.add_log_level,
        structlog.dev.ConsoleRenderer(),
    ],
    wrapper_class=structlog.make_filtering_bound_logger(20),
    logger_factory=structlog.PrintLoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

class ProcessingStatus(Enum):
    """√âtats de traitement des √©v√©nements"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"

@dataclass
class EventProcessingResult:
    """R√©sultat du traitement d'un √©v√©nement"""
    event_id: str
    status: ProcessingStatus
    ai_insights: Optional[Dict[str, Any]] = None
    processing_time: Optional[float] = None
    error_message: Optional[str] = None
    retry_count: int = 0
    processed_at: Optional[datetime] = None

@dataclass
class BatchProcessingStats:
    """Statistiques du traitement en batch"""
    total_events: int
    processed: int
    skipped: int
    failed: int
    processing_time: float
    insights_generated: int
    duplicates_avoided: int

class PhoenixEventQueueProcessor:
    """
    üéØ Processeur de queue d'√©v√©nements Phoenix avec garanties de qualit√©:
    
    ‚úÖ D√©duplication parfaite (SHA-256 hash)
    ‚úÖ Idempotence garantie (retry-safe)
    ‚úÖ Traitement batch intelligent
    ‚úÖ Monitoring complet
    ‚úÖ Fallback graceful
    """
    
    def __init__(self, supabase_url: str = None, supabase_key: str = None):
        """
        Initialise le processeur avec connexion Supabase s√©curis√©e
        
        Args:
            supabase_url: URL Supabase (env SUPABASE_URL si None)
            supabase_key: Cl√© Supabase (env SUPABASE_SERVICE_ROLE_KEY si None)
        """
        self.supabase_url = supabase_url or os.getenv("SUPABASE_URL")
        self.supabase_key = supabase_key or os.getenv("SUPABASE_SERVICE_ROLE_KEY")
        
        if not self.supabase_url or not self.supabase_key:
            raise ValueError("SUPABASE_URL et SUPABASE_SERVICE_ROLE_KEY requis")
        
        self.supabase: Client = create_client(self.supabase_url, self.supabase_key)
        
        # Cache des √©v√©nements trait√©s pour d√©duplication
        self._processed_hashes: Set[str] = set()
        self._processing_lock = asyncio.Lock()
        
        # Configuration
        self.batch_size = int(os.getenv("EVENT_BATCH_SIZE", "50"))
        self.max_retries = int(os.getenv("EVENT_MAX_RETRIES", "3"))
        self.processing_timeout = int(os.getenv("EVENT_PROCESSING_TIMEOUT", "30"))
        
        logger.info("‚úÖ PhoenixEventQueueProcessor initialis√©", 
                   batch_size=self.batch_size, 
                   max_retries=self.max_retries)
    
    def _generate_event_hash(self, event: Dict[str, Any]) -> str:
        """
        G√©n√®re un hash unique pour un √©v√©nement (d√©duplication)
        
        Args:
            event: Donn√©es de l'√©v√©nement
            
        Returns:
            str: Hash SHA-256 unique
        """
        # Cr√©er signature unique bas√©e sur champs critiques
        signature_data = {
            "stream_id": event.get("stream_id"),
            "event_type": event.get("event_type"), 
            "timestamp": event.get("timestamp"),
            "app_source": event.get("app_source"),
            # Hash du payload pour d√©tecter modifications
            "payload_hash": hashlib.sha256(
                str(event.get("payload", {})).encode()
            ).hexdigest()[:16]
        }
        
        # G√©n√©rer hash final
        signature_str = f"{signature_data['stream_id']}:{signature_data['event_type']}:{signature_data['timestamp']}:{signature_data['app_source']}:{signature_data['payload_hash']}"
        return hashlib.sha256(signature_str.encode()).hexdigest()
    
    async def _load_processed_hashes(self):
        """Charge les hashes des √©v√©nements d√©j√† trait√©s"""
        try:
            # R√©cup√©rer les √©v√©nements trait√©s des 7 derniers jours pour √©viter retraitement
            cutoff_date = datetime.now() - timedelta(days=7)
            
            response = self.supabase.table('phoenix_events')\
                .select('event_id, stream_id, event_type, timestamp, app_source, payload')\
                .not_.is_('ai_processed_at', 'null')\
                .gte('timestamp', cutoff_date.isoformat())\
                .execute()
            
            # G√©n√©rer hashes pour tous les √©v√©nements trait√©s
            for event in response.data:
                event_hash = self._generate_event_hash(event)
                self._processed_hashes.add(event_hash)
            
            logger.info(f"üìö Charg√© {len(self._processed_hashes)} hashes d'√©v√©nements trait√©s")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement hashes: {e}")
            # Continue sans cache - mode d√©grad√©
    
    async def get_unprocessed_events(self, limit: int = None) -> List[Dict[str, Any]]:
        """
        R√©cup√®re les √©v√©nements non trait√©s par IA de mani√®re s√ªre
        
        Args:
            limit: Nombre max d'√©v√©nements (batch_size par d√©faut)
            
        Returns:
            List[Dict]: √âv√©nements √† traiter
        """
        if limit is None:
            limit = self.batch_size
        
        try:
            # Requ√™te optimis√©e avec index
            response = self.supabase.table('phoenix_events')\
                .select('*')\
                .is_('ai_processed_at', 'null')\
                .is_('ai_processing_error', 'null')\
                .order('timestamp', desc=False)\
                .limit(limit)\
                .execute()
            
            events = response.data
            logger.info(f"üì• R√©cup√©r√© {len(events)} √©v√©nements non trait√©s")
            
            return events
            
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration √©v√©nements: {e}")
            return []
    
    async def _mark_event_processing(self, event_id: str, status: ProcessingStatus) -> bool:
        """
        Marque un √©v√©nement comme en cours de traitement (lock optimiste)
        
        Args:
            event_id: ID de l'√©v√©nement
            status: Nouveau statut
            
        Returns:
            bool: Succ√®s du verrouillage
        """
        try:
            if status == ProcessingStatus.PROCESSING:
                # Tentative de verrouillage optimiste
                response = self.supabase.table('phoenix_events')\
                    .update({
                        'ai_processing_started_at': datetime.now().isoformat(),
                        'ai_processing_status': status.value,
                        'ai_processing_instance': f"processor-{os.getpid()}"
                    })\
                    .eq('event_id', event_id)\
                    .is_('ai_processing_started_at', 'null')\
                    .execute()
                
                # V√©rifier si on a r√©ussi √† verrouiller
                return len(response.data) > 0
            
            else:
                # Mise √† jour finale
                update_data = {
                    'ai_processing_status': status.value,
                    'ai_processed_at': datetime.now().isoformat()
                }
                
                if status == ProcessingStatus.FAILED:
                    update_data['ai_processing_error'] = "Processing failed"
                
                self.supabase.table('phoenix_events')\
                    .update(update_data)\
                    .eq('event_id', event_id)\
                    .execute()
                
                return True
                
        except Exception as e:
            logger.error(f"‚ùå Erreur marquage √©v√©nement {event_id}: {e}")
            return False
    
    async def _generate_ai_insights(self, event: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        G√©n√®re des insights IA pour un √©v√©nement (sera connect√© aux agents)
        
        Args:
            event: Donn√©es de l'√©v√©nement
            
        Returns:
            Dict: Insights g√©n√©r√©s ou None si √©chec
        """
        try:
            # Pour l'instant, g√©n√©ration d'insights basiques
            # TODO: Connecter aux agents IA Docker
            
            insights = {
                "event_analysis": {
                    "event_type": event.get("event_type"),
                    "app_source": event.get("app_source"),
                    "user_engagement": self._calculate_engagement_score(event),
                    "cross_app_opportunities": self._detect_cross_app_opportunities(event)
                },
                "recommendations": {
                    "nudges": self._generate_nudge_recommendations(event),
                    "optimization": self._generate_optimization_suggestions(event)
                },
                "analytics": {
                    "processed_at": datetime.now().isoformat(),
                    "processing_version": "v1.0",
                    "confidence_score": 0.85
                }
            }
            
            return insights
            
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration insights pour {event.get('event_id')}: {e}")
            return None
    
    def _calculate_engagement_score(self, event: Dict[str, Any]) -> float:
        """Calcule un score d'engagement basique"""
        # Logique simple pour commencer
        event_type = event.get("event_type", "")
        
        scores = {
            "letter.generated": 0.8,
            "cv.uploaded": 0.7,
            "job_offer.analyzed": 0.9,
            "user.registered": 0.5
        }
        
        return scores.get(event_type, 0.3)
    
    def _detect_cross_app_opportunities(self, event: Dict[str, Any]) -> List[str]:
        """D√©tecte les opportunit√©s cross-app"""
        opportunities = []
        
        event_type = event.get("event_type", "")
        app_source = event.get("app_source", "")
        
        if app_source == "letters" and "letter.generated" in event_type:
            opportunities.append("cv_optimization_nudge")
            opportunities.append("rise_coaching_suggestion")
        
        elif app_source == "cv" and "cv.uploaded" in event_type:
            opportunities.append("letters_generation_nudge")
        
        return opportunities
    
    def _generate_nudge_recommendations(self, event: Dict[str, Any]) -> List[Dict[str, Any]]:
        """G√©n√®re des recommandations de nudges"""
        nudges = []
        
        opportunities = self._detect_cross_app_opportunities(event)
        
        for opportunity in opportunities:
            if opportunity == "cv_optimization_nudge":
                nudges.append({
                    "type": "cross_app_promotion",
                    "target_app": "cv",
                    "message": "Optimisez vos chances avec un CV ATS-friendly",
                    "priority": "high",
                    "timing": "immediate"
                })
        
        return nudges
    
    def _generate_optimization_suggestions(self, event: Dict[str, Any]) -> List[str]:
        """G√©n√®re des suggestions d'optimisation"""
        suggestions = []
        
        payload = event.get("payload", {})
        
        # Exemple: si g√©n√©ration de lettre longue
        if event.get("event_type") == "letter.generated":
            if isinstance(payload.get("letter_length"), int) and payload["letter_length"] > 2000:
                suggestions.append("consider_shorter_format")
        
        return suggestions
    
    async def process_event(self, event: Dict[str, Any]) -> EventProcessingResult:
        """
        Traite un √©v√©nement individuel avec toutes les garanties
        
        Args:
            event: √âv√©nement √† traiter
            
        Returns:
            EventProcessingResult: R√©sultat d√©taill√©
        """
        event_id = event.get("event_id")
        start_time = time.time()
        
        try:
            # 1. V√©rification d√©duplication
            event_hash = self._generate_event_hash(event)
            if event_hash in self._processed_hashes:
                logger.warning(f"üîÑ √âv√©nement {event_id} d√©j√† trait√© (hash: {event_hash[:8]})")
                return EventProcessingResult(
                    event_id=event_id,
                    status=ProcessingStatus.SKIPPED,
                    error_message="Duplicate event (hash match)"
                )
            
            # 2. Verrouillage optimiste
            if not await self._mark_event_processing(event_id, ProcessingStatus.PROCESSING):
                logger.warning(f"üîí √âv√©nement {event_id} d√©j√† en cours de traitement")
                return EventProcessingResult(
                    event_id=event_id,
                    status=ProcessingStatus.SKIPPED,
                    error_message="Already being processed"
                )
            
            # 3. G√©n√©ration insights IA
            insights = await asyncio.wait_for(
                self._generate_ai_insights(event),
                timeout=self.processing_timeout
            )
            
            if insights is None:
                await self._mark_event_processing(event_id, ProcessingStatus.FAILED)
                return EventProcessingResult(
                    event_id=event_id,
                    status=ProcessingStatus.FAILED,
                    processing_time=time.time() - start_time,
                    error_message="Failed to generate insights"
                )
            
            # 4. Sauvegarde des insights
            self.supabase.table('phoenix_events')\
                .update({
                    'ai_insights': insights,
                    'ai_processed_at': datetime.now().isoformat(),
                    'ai_processing_status': ProcessingStatus.COMPLETED.value
                })\
                .eq('event_id', event_id)\
                .execute()
            
            # 5. Mise √† jour cache d√©duplication
            self._processed_hashes.add(event_hash)
            
            # 6. Marquer comme compl√©t√©
            await self._mark_event_processing(event_id, ProcessingStatus.COMPLETED)
            
            processing_time = time.time() - start_time
            
            logger.info(f"‚úÖ √âv√©nement {event_id} trait√© avec succ√®s", 
                       processing_time=f"{processing_time:.2f}s")
            
            return EventProcessingResult(
                event_id=event_id,
                status=ProcessingStatus.COMPLETED,
                ai_insights=insights,
                processing_time=processing_time,
                processed_at=datetime.now()
            )
            
        except asyncio.TimeoutError:
            await self._mark_event_processing(event_id, ProcessingStatus.FAILED)
            return EventProcessingResult(
                event_id=event_id,
                status=ProcessingStatus.FAILED,
                processing_time=time.time() - start_time,
                error_message="Processing timeout"
            )
            
        except Exception as e:
            await self._mark_event_processing(event_id, ProcessingStatus.FAILED)
            logger.error(f"‚ùå Erreur traitement √©v√©nement {event_id}: {e}")
            return EventProcessingResult(
                event_id=event_id,
                status=ProcessingStatus.FAILED,
                processing_time=time.time() - start_time,
                error_message=str(e)
            )
    
    async def process_batch(self, events: List[Dict[str, Any]]) -> BatchProcessingStats:
        """
        Traite un batch d'√©v√©nements de mani√®re optimale
        
        Args:
            events: Liste des √©v√©nements √† traiter
            
        Returns:
            BatchProcessingStats: Statistiques compl√®tes
        """
        start_time = time.time()
        
        logger.info(f"üöÄ D√©marrage traitement batch de {len(events)} √©v√©nements")
        
        # Traitement concurrent avec limite
        semaphore = asyncio.Semaphore(5)  # Max 5 √©v√©nements simultan√©s
        
        async def process_with_semaphore(event):
            async with semaphore:
                return await self.process_event(event)
        
        # Ex√©cution parall√®le contr√¥l√©e
        results = await asyncio.gather(
            *[process_with_semaphore(event) for event in events],
            return_exceptions=True
        )
        
        # Analyse des r√©sultats
        processed = sum(1 for r in results if isinstance(r, EventProcessingResult) and r.status == ProcessingStatus.COMPLETED)
        skipped = sum(1 for r in results if isinstance(r, EventProcessingResult) and r.status == ProcessingStatus.SKIPPED)
        failed = sum(1 for r in results if isinstance(r, EventProcessingResult) and r.status == ProcessingStatus.FAILED)
        insights_generated = sum(1 for r in results if isinstance(r, EventProcessingResult) and r.ai_insights is not None)
        duplicates_avoided = sum(1 for r in results if isinstance(r, EventProcessingResult) and "Duplicate" in (r.error_message or ""))
        
        processing_time = time.time() - start_time
        
        stats = BatchProcessingStats(
            total_events=len(events),
            processed=processed,
            skipped=skipped,
            failed=failed,
            processing_time=processing_time,
            insights_generated=insights_generated,
            duplicates_avoided=duplicates_avoided
        )
        
        logger.info(f"üìä Batch trait√© en {processing_time:.2f}s", 
                   processed=processed, 
                   skipped=skipped, 
                   failed=failed,
                   insights=insights_generated)
        
        return stats
    
    async def run_processing_cycle(self) -> BatchProcessingStats:
        """
        Ex√©cute un cycle complet de traitement
        
        Returns:
            BatchProcessingStats: R√©sultats du cycle
        """
        async with self._processing_lock:
            # 1. Charger cache d√©duplication
            await self._load_processed_hashes()
            
            # 2. R√©cup√©rer √©v√©nements non trait√©s
            events = await self.get_unprocessed_events()
            
            if not events:
                logger.info("‚ú® Aucun √©v√©nement √† traiter - queue vide")
                return BatchProcessingStats(
                    total_events=0, processed=0, skipped=0, failed=0,
                    processing_time=0.0, insights_generated=0, duplicates_avoided=0
                )
            
            # 3. Traiter le batch
            stats = await self.process_batch(events)
            
            return stats
    
    async def start_continuous_processing(self, interval: int = 60):
        """
        D√©marre le traitement continu en arri√®re-plan
        
        Args:
            interval: Intervalle entre cycles (secondes)
        """
        logger.info(f"üîÑ D√©marrage traitement continu (intervalle: {interval}s)")
        
        while True:
            try:
                stats = await self.run_processing_cycle()
                
                if stats.total_events > 0:
                    logger.info(f"üìà Cycle termin√©: {stats.processed} trait√©s, {stats.failed} √©checs")
                
                await asyncio.sleep(interval)
                
            except KeyboardInterrupt:
                logger.info("üõë Arr√™t traitement continu")
                break
            except Exception as e:
                logger.error(f"‚ùå Erreur cycle traitement: {e}")
                await asyncio.sleep(interval * 2)  # Backoff en cas d'erreur

# ========================================
# üß™ FONCTIONS DE TEST ET VALIDATION
# ========================================

async def test_processor():
    """Test du processeur avec donn√©es factices"""
    try:
        processor = PhoenixEventQueueProcessor()
        
        # Test cycle de traitement
        stats = await processor.run_processing_cycle()
        
        print(f"‚úÖ Test r√©ussi: {stats.processed} √©v√©nements trait√©s")
        
    except Exception as e:
        print(f"‚ùå Test √©chou√©: {e}")

if __name__ == "__main__":
    # Test du processeur
    asyncio.run(test_processor())